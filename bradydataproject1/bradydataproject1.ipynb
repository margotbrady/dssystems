{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/margotbrady/dssystems/blob/main/bradydataproject1/bradydataproject1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project has been completed by Margot Brady (apn7yj)\n"
      ],
      "metadata": {
        "id": "cFdTKNlx1gBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Links to download the datasets:\n",
        "\n",
        "spongebob_episodes dataset : https://www.kaggle.com/datasets/myticalcat/spongebob-squarepants-episodes-dataset?select=spongebob_episodes.json\n",
        "\n",
        "exams dataset:https://www.kaggle.com/datasets/whenamancodes/students-performance-in-exams"
      ],
      "metadata": {
        "id": "Ezuc5YM21pIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are all of the libraries I used for the project:"
      ],
      "metadata": {
        "id": "bomlvEa5Dqk9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "d2ZMlmUvyWib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28375719-2f5b-420e-cd70-3831a2977561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.10/dist-packages (2.0.36)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "#install libraries\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "import ast\n",
        "!pip install SQLAlchemy\n",
        "from sqlalchemy import create_engine\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the 'extract' portion of the ETL project. It pulls the file name and path from the newest uploaded local file in colab, and then saves it into a dataframe."
      ],
      "metadata": {
        "id": "3yAxpqJfDvuH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "W16IpmCR03ON"
      },
      "outputs": [],
      "source": [
        "# Function to get the newest file uploaded\n",
        "def getfilepath(directory='/content/'):\n",
        "    # Get a list of all files in the directory with their full paths\n",
        "    files = glob.glob(directory + '*')\n",
        "\n",
        "    # Get all the files in the directory, not subdirectories\n",
        "    files = [i for i in files if os.path.isfile(i)]\n",
        "\n",
        "    # If no files are found, return None\n",
        "    if not files:\n",
        "        return None\n",
        "\n",
        "    # Find the most recently added file by using max() with modification time as the key\n",
        "    filepath = max(files, key=os.path.getmtime)\n",
        "\n",
        "    # Return the path of the newest file\n",
        "    return filepath\n",
        "\n",
        "\n",
        "# This is the 'extract' part, where we are fetching the data that was just uploaded\n",
        "\n",
        "def getdata():\n",
        "  filepath = getfilepath() #gets the filepath\n",
        "  try:\n",
        "    if filepath is None:\n",
        "      print(\"No files found in the directory.\")\n",
        "      return\n",
        "    print(f\"Newest file uploaded: {filepath}\") #confirming the file\n",
        "\n",
        "  # Determines the file type\n",
        "    if filepath.endswith('.json'):\n",
        "      filetype = 'JSON'\n",
        "      df = pd.read_json(filepath)\n",
        "    elif filepath.endswith('.csv'):\n",
        "      filetype = 'CSV'\n",
        "      df = pd.read_csv(filepath)\n",
        "    else:\n",
        "      # In case the user uploaded a different file type.\n",
        "      print(\"Unsupported file type. Please upload a JSON or CSV file.\")\n",
        "      return\n",
        "\n",
        "    #Show number of columns and rows in original datset uploaded\n",
        "    num_records = len(df)\n",
        "    num_columns = len(df.columns)\n",
        "    print(f\"Data Summary:\\n- Number of records: {num_records}\\n- Number of columns: {num_columns}\")\n",
        "    print(\"Columns in the dataset:\", df.columns.tolist())\n",
        "\n",
        "  #In case there's an error with the data\n",
        "  except ValueError as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None, None\n",
        "\n",
        "  return df, filepath,filetype  # Return both the dataframe and the filepath for future use"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This converts the datatype to the new requested file-type, and saves it locally."
      ],
      "metadata": {
        "id": "e4fmLogHEXUz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "SGYfBTMP1wjo"
      },
      "outputs": [],
      "source": [
        "#CSV converter, works for any file type because it will have been converted to a dataframe first\n",
        "def convert_to_csv(df, output_path):\n",
        "    df.to_csv(output_path, index=False)\n",
        "    #saves the data to a local file\n",
        "    print(f\"Data converted to CSV and saved at {output_path}\")\n",
        "\n",
        "def convert_to_json(df, output_path):\n",
        "#JSON converter, works for any file type because it will have been converted to a dataframe first\n",
        "    df.to_json(output_path, orient='records', lines=True)\n",
        "    print(f\"Data converted to JSON and saved at {output_path}\")\n",
        "\n",
        "def convert_to_sql():\n",
        "    #Convert columns with lists or dictionaries to JSON strings\n",
        "    for col in df.columns:\n",
        "        if df[col].apply(lambda x: isinstance(x, (list, dict))).any():\n",
        "            df[col] = df[col].apply(json.dumps)  # Convert lists/dicts to JSON strings\n",
        "    filename = input('What is the name of your file?')\n",
        "    engine = create_engine('sqlite:///' + filename + '_sql.db', echo=False)\n",
        "    df.to_sql(filename, con=engine, if_exists='replace', index=False)\n",
        "    print(f\"\\nFile converted successfully to SQL table as {filename}_converted.db!\")\n",
        "\n",
        "def transform():\n",
        "  while True:\n",
        "    #Asks for an output type\n",
        "    output = input(\"What data type would you like your data converted to? Please enter either 'JSON', 'SQL', or 'CSV'. \")\n",
        "\n",
        "    if datatype == output.upper():\n",
        "      print('Please enter a datatype different from your original file.')\n",
        "    elif output.upper() != datatype and output.upper() == 'CSV':\n",
        "      output_path = input(\"Please provide the path to save the converted file (e.g., /content/output.csv or output.json): \")\n",
        "      convert_to_csv(df, output_path)\n",
        "      break\n",
        "    elif output.upper()!= datatype and output.upper() == 'JSON':\n",
        "      output_path = input(\"Please provide the path to save the converted file (e.g., /content/output.csv or output.json): \")\n",
        "      convert_to_json(df, output_path)\n",
        "      break\n",
        "    elif output.upper()!=datatype and output.upper() == 'SQL':\n",
        "      convert_to_sql()\n",
        "      break\n",
        "    else:\n",
        "      print(\"\\nError: Unsupported output format selected.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first example uses the JSON Spongebob Episode dataset, and converts it to CSV."
      ],
      "metadata": {
        "id": "ib6Kl1On_ygz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example use: JSON to CSV\n",
        "\n",
        "\n",
        "print(\"Please upload your data source. It must be either a JSON or CSV file.\")\n",
        "\n",
        "#Fetch\n",
        "df, storedfilepath,datatype = getdata()\n",
        "print('\\nStored filepath: ' +storedfilepath)\n",
        "\n",
        "\n",
        "# The dataset loaded weirdly into the dataframe, so we need to take the information in the info column and create new columns.\n",
        "\n",
        "def parse_info_column(info):\n",
        "  if isinstance(info, str):\n",
        "    return ast.literal_eval(info)\n",
        "  return info\n",
        "\n",
        "# Apply the parsing function to the 'info' column\n",
        "df['info'] = df['info'].apply(parse_info_column)\n",
        "''\n",
        "# Expand the 'info' dictionary into separate columns\n",
        "df = pd.concat([df.drop(columns=['info']), df['info'].apply(pd.Series)], axis=1)\n",
        "\n",
        "\n",
        "#Show number of columns and rows in original datset uploaded\n",
        "num_records = len(df)\n",
        "num_columns = len(df.columns)\n",
        "print(f\"Data Summary:\\n- Number of records: {num_records}\\n- Number of columns: {num_columns}\")\n",
        "\n",
        "\n",
        "#transform:\n",
        "transform()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM2Zh58me1lB",
        "outputId": "bce8bfa3-7ede-4f62-8fab-48dafd246003"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your data source. It must be either a JSON or CSV file.\n",
            "Newest file uploaded: /content/spongebob_episodes.json\n",
            "Data Summary:\n",
            "- Number of records: 583\n",
            "- Number of columns: 4\n",
            "Columns in the dataset: ['title', 'info', 'characters', 'url']\n",
            "\n",
            "Stored filepath: /content/spongebob_episodes.json\n",
            "Data Summary:\n",
            "- Number of records: 583\n",
            "- Number of columns: 26\n",
            "What data type would you like your data converted to? Please enter either 'JSON', 'SQL', or 'CSV'. json\n",
            "Please enter a datatype different from your original file.\n",
            "What data type would you like your data converted to? Please enter either 'JSON', 'SQL', or 'CSV'. csv\n",
            "Please provide the path to save the converted file (e.g., /content/output.csv or output.json): /content/spongebob_episodes.csv\n",
            "Data converted to CSV and saved at /content/spongebob_episodes.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second example uses the student exam result dataset in the CSV format and converts it into SQL."
      ],
      "metadata": {
        "id": "WPh0u9XQ_u6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: CSV to SQL\n",
        "\n",
        "print(\"Please upload your data source. It must be either a JSON or CSV file.\")\n",
        "\n",
        "#Fetch\n",
        "df, storedfilepath,datatype = getdata()\n",
        "print('\\nStored filepath: ' +storedfilepath)\n",
        "\n",
        "#Add a column\n",
        "\n",
        "df['Exam_average'] =  df[['math score', 'reading score', 'writing score']].mean(axis=1)\n",
        "\n",
        "#transform:\n",
        "transform()\n",
        "\n",
        "#Show number of columns and rows in original datset uploaded\n",
        "num_records = len(df)\n",
        "num_columns = len(df.columns)\n",
        "print(f\"Data Summary:\\n- Number of records: {num_records}\\n- Number of columns: {num_columns}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKgQ9hckhdx_",
        "outputId": "641f47b9-90e3-4412-8199-31e272fc4349"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your data source. It must be either a JSON or CSV file.\n",
            "Newest file uploaded: /content/exams.csv\n",
            "Data Summary:\n",
            "- Number of records: 1000\n",
            "- Number of columns: 8\n",
            "Columns in the dataset: ['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course', 'math score', 'reading score', 'writing score']\n",
            "\n",
            "Stored filepath: /content/exams.csv\n",
            "What data type would you like your data converted to? Please enter either 'JSON', 'SQL', or 'CSV'. sql\n",
            "What is the name of your file?exams\n",
            "\n",
            "File converted successfully to SQL table as exams_converted.db!\n",
            "Data Summary:\n",
            "- Number of records: 1000\n",
            "- Number of columns: 9\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/pEesX3Ycc6l+epWIdrMc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}